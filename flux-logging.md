# Creating Job Logs using Flux's Python API

Flux offers a versatile and flexible Python API for generating job logs to be
forwarded to visualization tools such as Elastic.

## Fetching Job Data

As jobs are submitted to and run under Flux, they can be captured, listed, and
worked with in order to create logs specific to your use case; in CMVL's case,
we want to be able to create Flux job logs so that they can be forwarded to
visualization platforms for aggregation and analysis.

Fetching completed jobs requires a [Flux handle](https://flux-framework.readthedocs.io/projects/flux-core/en/latest/python/autogenerated/flux.core.handle.html#module-flux.core.handle),
which is used to connect to Flux. This handle is used when issuing an RPC to
Flux to actually query for completed jobs:

```python
rpc_handle = flux.job.job_list_inactive(handle, max_entries=0)
jobs = get_jobs(rpc_handle)
```

Additional arguments can be passed to the `job_list_inactive` call, such as a
seconds-since-epoch timestamp which will limit jobs to those which have only
been inactive since that timestamp. For all available optional arguments, see
the [job.list](https://flux-framework.readthedocs.io/projects/flux-core/en/latest/python/autogenerated/flux.job.list.html#flux.job.list.job_list_inactive)
module documentation.

Note that `max_entries=0` in the above code block means to grab _every_
available job.

The `jobs` object in the above code block will contain a list of dictionaries,
one dictionary per job. Each dictionary contains a set of basic information
about the job, such as the job ID, the user who ran the job, its state, size,
various timestamps, and more.

Each job's job ID can be used to query more detailed information about the job,
such as its [jobspec](https://flux-framework.readthedocs.io/projects/flux-rfc/en/latest/spec_14.html)
or [eventlog](https://flux-framework.readthedocs.io/projects/flux-rfc/en/latest/spec_18.html),
with another lookup to the [Key-Value Store (KVS)](https://flux-framework.readthedocs.io/en/latest/glossary.html#kvs):

```python
for job in jobs:
    job_data = flux.job.job_kvs_lookup(
        handle, job["id"], keys=["jobspec", "eventlog"], decode=True
    )
```

After a lookup, this list of dictionaries can be post-processed to be forwarded
to a visualization/monitoring platform for data aggregation and visualization.

Various groups of jobs can be created, such as jobs by final job state (e.g.
successful vs. failed jobs), total runtime, bank, queue, cluster, and more.
